{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import html\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codice_galattico = \"../Hackapizza Dataset/Codice Galattico/Codice Galattico.pdf\"\n",
    "manuale_cucina = \"../Hackapizza Dataset/Misc/Manuale di Cucina.pdf\"\n",
    "menu_esempio = \"../Hackapizza Dataset/Menu/Anima Cosmica.pdf\"\n",
    "\n",
    "domande = \"../Hackapizza Dataset/domande.csv\"\n",
    "piatti = \"../Hackapizza Dataset/Misc/dish_mapping.json\"\n",
    "pianeti = \"../Hackapizza Dataset/Misc/Distanze.csv\"\n",
    "\n",
    "blog_1 = \"../Hackapizza Dataset/Blogpost/blog_etere_del_gusto.html\"\n",
    "blog_2 = \"../Hackapizza Dataset/Blogpost/blog_sapore_del_dune.html\"\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Recupera endpoint e chiave dalle variabili d'ambiente\n",
    "api_base = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# neo4j first instance parameters\n",
    "NEO4J_URI= \"neo4j+s://0482640f.databases.neo4j.io\"\n",
    "NEO4J_USERNAME= \"neo4j\"\n",
    "NEO4J_PASSWORD= \"PNvdaZlk326-ja2hRD1K97ZUUMnD4mj0NsecZNu5-9k\"\n",
    "AURA_INSTANCEID= \"0482640f\"\n",
    "AURA_INSTANCENAME= \"Instance01\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crea_llm(api_key: str, api_base: str) -> AzureChatOpenAI:\n",
    "    \"\"\"\n",
    "    Crea un'istanza del modello linguistico Azure OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): La chiave API di Azure\n",
    "        api_base (str): L'URL base dell'API Azure\n",
    "        \n",
    "    Returns:\n",
    "        AzureChatOpenAI: Istanza configurata del modello\n",
    "    \"\"\"\n",
    "    return AzureChatOpenAI(\n",
    "        openai_api_version=\"2024-08-01-preview\",\n",
    "        azure_deployment=\"o1-mini\",\n",
    "        azure_endpoint=api_base,\n",
    "        api_key=api_key,\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "llm = crea_llm(api_key, api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estrai_testo_pdf(percorso_pdf):\n",
    "    with open(percorso_pdf, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        testo = \"\"\n",
    "        for pagina in reader.pages:\n",
    "            testo += pagina.extract_text()\n",
    "    return testo\n",
    "\n",
    "def estrai_testo_html(percorso_html):\n",
    "    \"\"\"Estrae il testo da un file HTML.\"\"\"\n",
    "    with open(percorso_html, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        return soup.get_text(separator=\" \", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METODI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json_repair import repair_json\n",
    "\n",
    "def estrai_relazioni(testo, relazioni_info, llm, driver):\n",
    "    \"\"\"\n",
    "    Estrae entità e relazioni tra di esse da un testo.\n",
    "    \n",
    "    Args:\n",
    "        testo: Il testo da cui estrarre le relazioni\n",
    "        relazioni_info: Dizionario nella forma {'NOME_RELAZIONE': ['Tipo_Entità1', 'Tipo_Entità2']}\n",
    "        llm: Modello di linguaggio da utilizzare\n",
    "        driver: Driver Neo4j\n",
    "        \n",
    "    Returns:\n",
    "        Dizionario contenente entità e relazioni estratte\n",
    "    \"\"\"\n",
    "    risultati = {}\n",
    "    entita_tipi = set()\n",
    "    \n",
    "    # Raccogli tutti i tipi di entità unici dalle relazioni\n",
    "    for _, [tipo_entita1, tipo_entita2] in relazioni_info.items():\n",
    "        entita_tipi.add(tipo_entita1)\n",
    "        entita_tipi.add(tipo_entita2)\n",
    "    \n",
    "    # Estrai tutte le entità una sola volta per tipo\n",
    "    for tipo_entita in entita_tipi:\n",
    "        print(f\"Estraendo entità di tipo '{tipo_entita}'...\")\n",
    "        entita = estrai_valorizzazioni(testo, tipo_entita, llm, driver)\n",
    "        risultati[tipo_entita] = entita\n",
    "    \n",
    "    # Ora estrai le relazioni\n",
    "    for nome_relazione, [tipo_entita1, tipo_entita2] in relazioni_info.items():\n",
    "        print(f\"Estraendo relazioni '{nome_relazione}' tra '{tipo_entita1}' e '{tipo_entita2}'...\")\n",
    "        \n",
    "        # Estrai le relazioni tra entità1 e entità2\n",
    "        relazioni = estrai_valorizzazioni_relazioni(\n",
    "            testo, \n",
    "            nome_relazione, \n",
    "            tipo_entita1, \n",
    "            tipo_entita2, \n",
    "            risultati[tipo_entita1], \n",
    "            risultati[tipo_entita2], \n",
    "            llm, \n",
    "            driver\n",
    "        )\n",
    "        \n",
    "        # Salva le relazioni nel risultato\n",
    "        risultati[nome_relazione] = relazioni\n",
    "        \n",
    "    return risultati\n",
    "\n",
    "def estrai_valorizzazioni(testo, categoria, llm, driver):\n",
    "    \"\"\"\n",
    "    Estrae le valorizzazioni di entità di una specifica categoria dal testo.\n",
    "    \"\"\"\n",
    "    # Genera un prompt basato sulle entità filtrate     \n",
    "    prompt = f\"\"\"\n",
    "    Contesto: questo testo rientra in una collezione di documenti che parlano di Ristoranti nello spazio (su Pianeti) e sui loro Menu, Piatti, Ingredienti, Tecniche e Licenze.\n",
    "    \n",
    "    Il testo che segue potrebbe contenere informazioni pertinenti a questa categoria: {categoria}\n",
    "    In output voglio tutte le possibile istanze di questa categoria, come entità da caricare su neo4j.\n",
    "    Per ogni istanza recupera:\n",
    "        - NOME dell'istanza\n",
    "        - eventuali PARAMETRI associati all'istanza\n",
    "    Attento che i nomi delle chiavi devono essere identiche a quelle fornite.\n",
    "    \n",
    "    Restituisci SOLO il JSON valido e completo, senza formattazione markdown o decoratori.\n",
    "    \n",
    "    Testo:\n",
    "    {testo}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chiamata al modello LLM per ottenere la valorizzazione delle entità\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "    \n",
    "    # Pulizia del JSON\n",
    "    if content.startswith('```') and content.endswith('```'):\n",
    "        content = '\\n'.join(content.split('\\n')[1:-1])\n",
    "    if content.startswith('```json'):\n",
    "        content = content[7:].strip()\n",
    "    elif content.startswith('json'):\n",
    "        content = content[4:].strip()\n",
    "    \n",
    "    # Tentativo di riparare il JSON\n",
    "    try:\n",
    "        valorizzazioni = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing JSON per entità {categoria}: {str(e)}\")\n",
    "        try:\n",
    "            valorizzazioni = json.loads(repair_json(content))\n",
    "        except Exception:\n",
    "            # Fallback con richiesta all'LLM\n",
    "            prompt_repair = f\"Correggi questo JSON per Neo4j e assicurati che sia valido e completo:\\n{content}\"\n",
    "            repaired = llm.invoke(prompt_repair).content.strip()\n",
    "            \n",
    "            # Pulizia del JSON riparato\n",
    "            if repaired.startswith('```') and repaired.endswith('```'):\n",
    "                repaired = '\\n'.join(repaired.split('\\n')[1:-1])\n",
    "            if repaired.startswith('```json'):\n",
    "                repaired = repaired[7:].strip()\n",
    "            elif repaired.startswith('json'):\n",
    "                repaired = repaired[4:].strip()\n",
    "                \n",
    "            try:\n",
    "                valorizzazioni = json.loads(repaired)\n",
    "            except:\n",
    "                print(f\"Impossibile riparare il JSON per le entità di tipo {categoria}\")\n",
    "                valorizzazioni = []\n",
    "    \n",
    "    # Assicurati che il risultato sia una lista\n",
    "    if not isinstance(valorizzazioni, list):\n",
    "        valorizzazioni = [valorizzazioni]\n",
    "    \n",
    "    return valorizzazioni\n",
    "\n",
    "def estrai_valorizzazioni_relazioni(testo, nome_relazione, tipo_entita1, tipo_entita2, entita1, entita2, llm, driver):\n",
    "    \"\"\"\n",
    "    Estrae le relazioni specifiche tra entità di due tipi.\n",
    "    \"\"\"\n",
    "    # Genera un prompt per estrarre le relazioni\n",
    "    prompt = f\"\"\"\n",
    "    Contesto: questo testo rientra in una collezione di documenti che parlano di Ristoranti nello spazio (su Pianeti) e sui loro Menu, Piatti, Ingredienti, Tecniche e Licenze.\n",
    "    \n",
    "    Il testo che segue potrebbe contenere informazioni su relazioni di tipo '{nome_relazione}' tra entità di tipo '{tipo_entita1}' e '{tipo_entita2}'.\n",
    "    \n",
    "    In output voglio tutte le relazioni di tipo '{nome_relazione}' come triple da caricare su neo4j.\n",
    "    Per ogni relazione recupera:\n",
    "        - ORIGINE: nome dell'entità di tipo '{tipo_entita1}'\n",
    "        - DESTINAZIONE: nome dell'entità di tipo '{tipo_entita2}'\n",
    "        - eventuali PARAMETRI associati alla relazione\n",
    "    \n",
    "    Ecco le entità di tipo '{tipo_entita1}' già identificate:\n",
    "    {[e.get(\"NOME\", \"\") for e in (entita1 if isinstance(entita1, list) else [entita1])]}\n",
    "    \n",
    "    Ecco le entità di tipo '{tipo_entita2}' già identificate:\n",
    "    {[e.get(\"NOME\", \"\") for e in (entita2 if isinstance(entita2, list) else [entita2])]}\n",
    "    \n",
    "    Restituisci SOLO il JSON valido e completo, senza formattazione markdown o decoratori.\n",
    "    Il formato deve essere:\n",
    "    [\n",
    "      {{\"ORIGINE\": \"nome_entita1\", \"DESTINAZIONE\": \"nome_entita2\", \"PARAMETRI\": {{...parametri...}} }}\n",
    "    ]\n",
    "    \n",
    "    Testo:\n",
    "    {testo}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chiamata al modello LLM per ottenere la valorizzazione delle relazioni\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "    \n",
    "    # Pulizia del JSON\n",
    "    if content.startswith('```') and content.endswith('```'):\n",
    "        content = '\\n'.join(content.split('\\n')[1:-1])\n",
    "    if content.startswith('```json'):\n",
    "        content = content[7:].strip()\n",
    "    elif content.startswith('json'):\n",
    "        content = content[4:].strip()\n",
    "    \n",
    "    # Tentativo di riparare il JSON\n",
    "    try:\n",
    "        relazioni = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing JSON delle relazioni {nome_relazione}: {str(e)}\")\n",
    "        try:\n",
    "            relazioni = json.loads(repair_json(content))\n",
    "        except Exception:\n",
    "            # Fallback con richiesta all'LLM\n",
    "            prompt_repair = f\"Correggi questo JSON per Neo4j e assicurati che sia valido e completo:\\n{content}\"\n",
    "            repaired = llm.invoke(prompt_repair).content.strip()\n",
    "            \n",
    "            # Pulizia del JSON riparato\n",
    "            if repaired.startswith('```') and repaired.endswith('```'):\n",
    "                repaired = '\\n'.join(repaired.split('\\n')[1:-1])\n",
    "            if repaired.startswith('```json'):\n",
    "                repaired = repaired[7:].strip()\n",
    "            elif repaired.startswith('json'):\n",
    "                repaired = repaired[4:].strip()\n",
    "                \n",
    "            try:\n",
    "                relazioni = json.loads(repaired)\n",
    "            except:\n",
    "                print(f\"Impossibile riparare il JSON delle relazioni {nome_relazione}\")\n",
    "                relazioni = []\n",
    "    \n",
    "    # Assicurati che il risultato sia una lista\n",
    "    if not isinstance(relazioni, list):\n",
    "        relazioni = [relazioni]\n",
    "    \n",
    "    return relazioni\n",
    "\n",
    "def carica_entita_e_relazioni_su_neo4j(risultati, relazioni_info, driver):\n",
    "    \"\"\"\n",
    "    Carica entità e relazioni su Neo4j\n",
    "    \n",
    "    Args:\n",
    "        risultati: Dizionario contenente entità e relazioni\n",
    "        relazioni_info: Dizionario delle relazioni come riferimento\n",
    "        driver: Driver Neo4j\n",
    "    \"\"\"\n",
    "    # Crea un set di tutti i tipi di relazione\n",
    "    tipi_relazione = set(relazioni_info.keys())\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Per ogni tipo di entità nel risultato\n",
    "        for tipo_entita, entita_list in risultati.items():\n",
    "            # Salta le chiavi che sono nomi di relazione\n",
    "            if tipo_entita in tipi_relazione:\n",
    "                continue\n",
    "                \n",
    "            # Carica le entità\n",
    "            print(f\"Caricando {len(entita_list) if isinstance(entita_list, list) else 1} entità di tipo {tipo_entita}...\")\n",
    "            \n",
    "            entita_array = entita_list if isinstance(entita_list, list) else [entita_list]\n",
    "            for entita in entita_array:\n",
    "                nome = entita.get(\"NOME\", \"\")\n",
    "                if not nome:  # Salta entità senza nome\n",
    "                    continue\n",
    "                    \n",
    "                parametri = entita.get(\"PARAMETRI\", {})\n",
    "                \n",
    "                # Sanificazione dei parametri per Neo4j (escape di apici)\n",
    "                sanitized_params = {}\n",
    "                for k, v in parametri.items():\n",
    "                    if isinstance(v, str):\n",
    "                        sanitized_params[k] = v.replace(\"'\", \"\\\\'\")\n",
    "                    else:\n",
    "                        sanitized_params[k] = v\n",
    "                \n",
    "                # Converti i parametri in formato per Cypher\n",
    "                params_string = \", \".join([f\"{k}: '{v}'\" for k, v in sanitized_params.items()])\n",
    "                \n",
    "                # Query Cypher per creare l'entità\n",
    "                query = f\"\"\"\n",
    "                MERGE (e:{tipo_entita} {{nome: $nome}})\n",
    "                \"\"\"\n",
    "                \n",
    "                if params_string:\n",
    "                    query += f\"\"\"\n",
    "                    SET e += {{{params_string}}}\n",
    "                    \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    session.run(query, nome=nome)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento dell'entità {nome}: {str(e)}\")\n",
    "        \n",
    "        # Per ogni tipo di relazione nel risultato\n",
    "        for tipo_relazione, relazioni in risultati.items():\n",
    "            # Salta le chiavi che non sono tipi di relazione\n",
    "            if tipo_relazione not in tipi_relazione:\n",
    "                continue\n",
    "                \n",
    "            # Determina i tipi di entità coinvolti nella relazione\n",
    "            tipo_origine, tipo_destinazione = relazioni_info[tipo_relazione]\n",
    "            \n",
    "            # Carica le relazioni\n",
    "            print(f\"Caricando {len(relazioni)} relazioni di tipo {tipo_relazione}...\")\n",
    "            \n",
    "            for relazione in relazioni:\n",
    "                origine = relazione.get(\"ORIGINE\", \"\")\n",
    "                destinazione = relazione.get(\"DESTINAZIONE\", \"\")\n",
    "                \n",
    "                if not origine or not destinazione:  # Salta relazioni incomplete\n",
    "                    continue\n",
    "                    \n",
    "                parametri = relazione.get(\"PARAMETRI\", {})\n",
    "                \n",
    "                # Sanificazione dei parametri per Neo4j\n",
    "                sanitized_params = {}\n",
    "                for k, v in parametri.items():\n",
    "                    if isinstance(v, str):\n",
    "                        sanitized_params[k] = v.replace(\"'\", \"\\\\'\")\n",
    "                    else:\n",
    "                        sanitized_params[k] = v\n",
    "                \n",
    "                # Converti i parametri in formato per Cypher\n",
    "                params_string = \", \".join([f\"{k}: '{v}'\" for k, v in sanitized_params.items()])\n",
    "                \n",
    "                # Query Cypher per creare la relazione\n",
    "                query = f\"\"\"\n",
    "                MATCH (o:{tipo_origine} {{nome: $origine}}), (d:{tipo_destinazione} {{nome: $destinazione}})\n",
    "                MERGE (o)-[r:{tipo_relazione}\"\"\"\n",
    "                \n",
    "                if params_string:\n",
    "                    query += f\" {{{params_string}}}\"\n",
    "                \n",
    "                query += \"]->(d)\"\n",
    "                \n",
    "                try:\n",
    "                    session.run(query, origine=origine, destinazione=destinazione)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento della relazione {origine}-[{tipo_relazione}]->{destinazione}: {str(e)}\")\n",
    "\n",
    "# Funzione principale per processare un testo completo\n",
    "def processa_testo_completo(testo, relazioni_info, llm, driver):\n",
    "    \"\"\"\n",
    "    Processa un testo estraendo entità e relazioni e caricandole su Neo4j.\n",
    "    \n",
    "    Args:\n",
    "        testo: Testo da processare\n",
    "        relazioni_info: Dizionario della forma {'NOME_RELAZIONE': ['Tipo_Entità1', 'Tipo_Entità2']}\n",
    "        llm: Modello di linguaggio\n",
    "        driver: Driver Neo4j\n",
    "        \n",
    "    Returns:\n",
    "        Dizionario contenente le entità e relazioni estratte\n",
    "    \"\"\"\n",
    "    # Estrai entità e relazioni\n",
    "    risultati = estrai_relazioni(testo, relazioni_info, llm, driver)\n",
    "    \n",
    "    # Carica tutto su Neo4j\n",
    "    carica_entita_e_relazioni_su_neo4j(risultati, relazioni_info, driver)\n",
    "    \n",
    "    return risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tecnica', 'Licenza', 'Ordine', 'Ingrediente', 'Piatto', 'Ristorante', 'Pianeta']\n"
     ]
    }
   ],
   "source": [
    "with open('entita_filtrate.txt', 'r', encoding='utf-8') as file:\n",
    "    entita_filtrate = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(entita_filtrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità CSV: {'CONTIENE': ['Piatto', 'Ingrediente'], 'PREPARATO': ['Piatto', 'Tecnica'], 'OFFERTA': ['Ristorante', 'Piatto'], 'SITUATO': ['Ristorante', 'Pianeta'], 'RICHIEDE': ['Piatto', 'Licenza'], 'APPARTIENE': ['Ordine', 'Piatto']}\n"
     ]
    }
   ],
   "source": [
    "# Carica il file CSV\n",
    "df_domande = pd.read_csv(domande)\n",
    "testo_domande = df_domande.to_string()  # Converti il CSV in testo\n",
    "entita_domande = estrai_relazioni(testo=testo_domande, categorie=entita_filtrate, llm=llm)\n",
    "print(\"Entità CSV:\", entita_domande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità CSV: {'DISTANTE': ['Pianeta', 'Pianeta']}\n"
     ]
    }
   ],
   "source": [
    "# Carica il file CSV\n",
    "df_pianeti = pd.read_csv(pianeti)\n",
    "testo_pianeti = df_pianeti.to_string()  # Converti il CSV in testo\n",
    "entita_pianeti = estrai_relazioni(testo=testo_pianeti, categorie=entita_filtrate, llm=llm)\n",
    "print(\"Entità CSV:\", entita_pianeti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità PDF: {'utilizza': ['Tecnica', 'Ingrediente'], 'prepara': ['Tecnica', 'Piatto'], 'contiene': ['Piatto', 'Ingrediente'], 'situato': ['Ristorante', 'Pianeta'], 'serve': ['Ristorante', 'Piatto'], 'richiede': ['Tecnica', 'Licenza'], 'appartiene_piatto_ord': ['Piatto', 'Ordine'], 'appartiene_ristorante_ord': ['Ristorante', 'Ordine'], 'regola': ['Ordine', 'Licenza'], 'implementa': ['Ordine', 'Tecnica'], 'influenzato': ['Piatto', 'Pianeta']}\n"
     ]
    }
   ],
   "source": [
    "# Esegui l'analisi del PDF\n",
    "testo_manuale = estrai_testo_pdf(manuale_cucina)\n",
    "entita_manuale = estrai_relazioni(testo=testo_manuale, categorie=entita_filtrate, llm=llm)\n",
    "print(\"Entità PDF:\", entita_manuale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità PDF: {'requires': ['Tecnica', 'Licenza'], 'belongs_to': ['Ordine', 'Pianeta'], 'prepares': ['Tecnica', 'Piatto'], 'located_on': ['Ristorante', 'Pianeta'], 'contains': ['Piatto', 'Ingrediente'], 'regulated_by': ['Ingrediente', 'Licenza'], 'implements': ['Ristorante', 'Tecnica'], 'follows': ['Ordine', 'Tecnica']}\n"
     ]
    }
   ],
   "source": [
    "# Esegui l'analisi del PDF\n",
    "testo_codice = estrai_testo_pdf(codice_galattico)\n",
    "entita_codice = estrai_relazioni(testo=testo_codice, categorie=entita_filtrate, llm=llm)\n",
    "print(\"Entità PDF:\", entita_codice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità PDF: {'contiene': [['Ristorante', 'Piatto'], ['Piatto', 'Ingrediente']], 'preparato': [['Piatto', 'Tecnica']], 'situato': [['Ristorante', 'Pianeta']], 'possiede': [['Ristorante', 'Licenza']]}\n"
     ]
    }
   ],
   "source": [
    "# Esegui l'analisi del PDF\n",
    "testo_menu = estrai_testo_pdf(menu_esempio)\n",
    "entita_menu = estrai_relazioni(testo=testo_menu, categorie=entita_filtrate, llm=llm)\n",
    "print(\"Entità PDF:\", entita_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità PDF: {'Ristorante': 35.0, 'Piatti': 25.0, 'Ingrediente': 20.0, 'Chef': 10.0, 'Tecnica': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo:\n",
    "testo_blog1 = estrai_testo_html(blog_1)\n",
    "entita_blog1 = estrai_schema(testo=testo_blog1, llm=llm)\n",
    "print(\"Entità PDF:\", entita_blog1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entità PDF: {'Ristorante': 30.0, 'Ingrediente': 25.0, 'Piatto': 20.0, 'Tecnica': 10.0, 'Chef': 5.0, 'Pianeta': 5.0, 'Critico': 3.0, 'Voto': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo:\n",
    "testo_blog2 = estrai_testo_html(blog_2)\n",
    "entita_blog2 = estrai_schema(testo=testo_blog2, llm=llm)\n",
    "print(\"Entità PDF:\", entita_blog2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi combinati: {'Sostanza': 30.0, 'Tecnica': 26.25, 'Licenza': 22.75, 'Ordine': 10.0, 'Ingrediente': 28.125, 'Piatto': 40.0, 'Ristorante': 29.0, 'Pianeta': 40.0, 'Cottura': 15.0, 'Preparazione': 8.0, 'Galassia': 5.0, 'Menu': 12.0, 'Chef': 8.333333333333334, 'Critico': 3.0, 'Voto': 2.0}\n"
     ]
    }
   ],
   "source": [
    "json_list = [entita_codice, entita_domande, entita_manuale, entita_menu, entita_pianeti, entita_blog1, entita_blog2]\n",
    "punteggi_combinati = combina_punteggi(json_list, llm)\n",
    "print(\"Punteggi combinati:\", punteggi_combinati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sostanza', 'Tecnica', 'Licenza', 'Ordine', 'Ingrediente', 'Piatto', 'Ristorante', 'Pianeta', 'Cottura', 'Menu']\n"
     ]
    }
   ],
   "source": [
    "# Filtra le entità con punteggio superiore a 20\n",
    "entita_filtrate = [entita for entita, punteggio in punteggi_combinati.items() if punteggio >= 10]\n",
    "\n",
    "print(entita_filtrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m entita_filtrate\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSostanza\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mentita_filtrate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMenu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m entita_filtrate\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCottura\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "entita_filtrate.remove('Sostanza')\n",
    "entita_filtrate.remove('Menu')\n",
    "entita_filtrate.remove('Cottura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entita_filtrate.txt', 'w') as file:\n",
    "    for entita in entita_filtrate:\n",
    "        file.write(f\"{entita}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
